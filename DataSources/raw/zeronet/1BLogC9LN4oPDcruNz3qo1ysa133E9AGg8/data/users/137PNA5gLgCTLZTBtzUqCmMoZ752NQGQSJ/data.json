{
	"next_comment_id": 6,
	"comment": [
		{
			"comment_id": 1,
			"body": "For big files is not preferable to have native integration with IPFS?\nIPFS already has a very good way of storing and seeding big files. I see IPFS and ZeroNet as systems that complement each other. \nZeroNet provides the decentralized DB storage and IPFS file storage.\nOn Antilibrary.bit I'm already storing most book files on IPFS and as such they are downloadable from the IPFS gateway.",
			"post_id": 95,
			"date_added": 1475699429
		},
		{
			"comment_id": 2,
			"body": "> [nofish](#comment_128_1J3rJ8ecnwH2EPYa6MrgZttBNc61ACFiCj): Built-in torrent support also planned in the future. (It's works well, battle-tested and has tons of content)\n\nThat's cool. The main difference I see though is that torrent is good for big files, IPFS is good for files in general. For example, if you want to share a 100 files, the way to do that with torrent is to either create one torrent pack with all 100 files in it or create a 100 torrent files, one for each file you want to share.\nOn IPFS you can simply share the whole folder and each file can be downloaded separately. Much like a windows shared folder or an ftp.\nIn the case of images for example, if you have an imgur clone on zeronet, you don't want to host all images of the zite inside zeronet or else the zite would be, in time, too big for anyone to hold it. You also cannot host the images on the torrent network and show them on the site. \n\nIPFS is a good match in these cases, all images for the zite could be hosted on the IPFS network and when the user requests an image on the zite it will load the image either from a native implementation of the ipfs protocol or from the [IPFS gateway](https://gateway.ipfs.io/ipfs/QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB/) by using a normal <img> tag.\nUsers would have the option to host those images or 'donate' hard drive space to that zite.\nThe 'donate space' model makes more sense in these scenarios where you have lot's of small files to share because we can have a smart space allocation of the total donated space, and by not asking for individual files to be seeded on a torrent client the zite can use the space to seed files that are in the long-tail. (IPFS will also let users mount folders from the network, which is super cool and useful as well)\n\nI'm actually doing that with the antilibrary zite. Users will be able to donate hard drive space and the zite will make a script available that will interact with the DB of books (that is inside the data dir) and make a smart usage of the space donated. The problem I'm facing is that, by not having anything native on zeronet, users will be expected to run a 'worker' node of the zite, and the node will do the work of downloading the files and seeding them.\n",
			"post_id": 95,
			"date_added": 1475829838
		},
		{
			"comment_id": 3,
			"body": "Thank you for the time and effort you're putting into this great project! ZeroNet is already making the internet a better place for us. \nWe're now all curious about this secret project...",
			"post_id": 102,
			"date_added": 1484340343
		},
		{
			"comment_id": 4,
			"body": "\nI'm very happy for you nofish!\n\nFred Brooks said: *The programmer, like the poet, works only slightly removed from pure thought-stuff. He builds his castles in the air, from air, creating by exertion of the imagination.*\n\nAnd from that thought-stuff you are changing your life and the internet for the better. Congrats!\n\nI agree with you when you said you need to tell a story. Humans love stories.\n\nSome ideas that could be developed into stories:\n\n- You could rescue the original intent of the web as envisioned by it's creator Tim Berners-Lee. He said *\"I want a web that's open, works internationally, works as well as possible and is not nation-based\"*. Now, this is not congruent with the current concept of web servers. The original internet idea was for everyone that was plugged into the network to be it's own server. Everyone would be able to host their own site. For reasons that seem to be obvious now that didn't work out as planned (eg: security, home servers would crash with quantity of visitors, etc). ZeroNet is making that possible. It is open because anyone can clone an existing website and make it his own. It is international because it is already translated into many languages and used by people in all these countries. It is not nation-based because it is in every nation.\n- You could also mention how some technologies are only possible when the time is right, when the necessary technologies are available and they are put together in the right way. The iphone was not the first smartphone nor touchscreen phone. The same goes for the ipod and many other technologies. The original objective of a decentralised democratic network was not possible until now because only now we have the required technology to build it. And ZeroNet is the iteration that is packaging all of it in the right way.\n- There was a time when the web was fun. Everyone had a funny looking website about whatever they wanted (think of geocities). As capitalism discovered the web, it became a more business-like place. A more boring place. ZeroNet makes it fun again by giving back to people control over the way they express themselves online. Suddenly it is possible again for you to have a site about whatever you want looking the way you want it to look like.\n- Maybe you can mention the 'offline' aspect of zeronet. Imagine if you could download the sites you most like on the internet. If you could access and use them without an internet connection, without loading time, and when you connected to the internet your computer would sync the site with the latest content available. Internet is a reality in many places now, but still 60% of the world population lack access to the internet. ZeroNet in this sense makes the internet inclusive again, giving these people access to information even without an always available internet connection. What if governments adopt the use of the technology and start to create How-to and libraries websites for people in remote locations so that they get a computer with the sites already downloaded and available, and once in a while they can take the computer to the nearest city with internet access to 'sync' the sites. They could have basic medical information, agricultural, encyclopedia, etc. ZeroNet becomes the ultimate equalizer, giving people without an internet connection access to information. Just like books did once for people that had no physical access to universities/libraries.\n- The book story parallel can also be good. Once upon a time we had medieval universities. Books were rare. Knowledge was rare. There was no such a thing as auto-didactictism. Then books started to become cheap, you start to have libraries, knowledge starts to become accessible. You still needed to have a library near home to be able to access all that knowledge. Then comes the internet, the most powerful weapon of knowledge. Everyone with a computer plugged into the internet can access all that knowledge. So, at the current stage of the internet, it is as it was when you had libraries, you could access all that knowledge only if you had access to a library. With zeronet that's not true anymore. ZeroNet is like a magic wand that would let you go to the nearest library and clone that whole library and place the clone in your home, for you to access it whenever you wanted. Everyone can have all that knowledge in their home computers.\n- Another possible source of ideas may be that, because of the way computing storage is expanding fast, in a long enough time frame everyone will be able to have a copy of the 'internet' in their own computers. Today you can literally fit a whole library into a hard drive. That was unthinkable 50 years ago. The internet might evolve to the same architecture of ZeroNet. When we become an interplanetary specie there will always be lags between the distant planets and as such content will always need to be available locally and it will sync in the background. Investing in an architecture that assumes eventual connection is better and more robust than an architecture that assumes constant connectivity.r ",
			"post_id": 107,
			"date_added": 1489225897
		},
		{
			"comment_id": 5,
			"body": "@nofish would you mind to expand on 'Support optional file database files'?\nWhat's the documentation page for that?",
			"post_id": 108,
			"date_added": 1492184859
		}
	],
	"comment_vote": {},
	"post_vote": {
		"94": 1
	}
}